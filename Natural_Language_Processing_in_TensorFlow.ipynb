{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdanALalawni/NLP/blob/main/Natural_Language_Processing_in_TensorFlow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmCn6Ze6ZBeZ"
      },
      "source": [
        "## **Week1 (sentiment in text)**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2mvPsJAFUfGr"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1oqA6wF7xV9",
        "outputId": "11b949ae-a146-430c-e245-be007ae145c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.5.16)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.6)\n",
            "Collecting opendatasets\n",
            "  Downloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from opendatasets) (4.66.1)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (from opendatasets) (1.5.16)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from opendatasets) (8.1.7)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.31.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (8.0.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle->opendatasets) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (3.6)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# prompt: load json dataset from kaggle directlly\n",
        "\n",
        "!pip install kaggle\n",
        "!pip install opendatasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BsyRSf73DdYC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "outputId": "43b90278-dd84-4d4b-e8f5-2245852aacc8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'opendatasets'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-d93a2fe27d63>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mopendatasets\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'opendatasets'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import opendatasets as od\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "go_TTqvQCamP"
      },
      "outputs": [],
      "source": [
        "od.download(\"https://www.kaggle.com/datasets/rmisra/news-headlines-dataset-for-sarcasm-detection/data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPIDehBUktk_"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-C5wVZ2Kraw"
      },
      "outputs": [],
      "source": [
        "def tok(s):\n",
        "  token=Tokenizer(oov_token=\"<OOV>\")\n",
        "  token.fit_on_texts(s)\n",
        "  word_index=token.word_index\n",
        "  seq=token.texts_to_sequences(s)\n",
        "  print(word_index)\n",
        "  return pad_sequences(seq,padding='post',truncating='post')\n",
        "s={\n",
        "    'I really love my dog',\n",
        "    'I love my cat',\n",
        "    'i think my dog is amazing also its prety'\n",
        "}\n",
        "\n",
        "print(tok(s))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1chzTYwFVP-A"
      },
      "outputs": [],
      "source": [
        "s={\n",
        "    'I really love my dog',\n",
        "    'I love my cat',\n",
        "    'i think my dog is amazing also its prety'\n",
        "}\n",
        "token=Tokenizer(num_words=100,oov_token=\"<'OOV'>\")\n",
        "token.fit_on_texts(s)\n",
        "word_index=token.word_index\n",
        "seq=token.texts_to_sequences(s)\n",
        "pad=pad_sequences(seq,padding='post',truncating='post')\n",
        "print(word_index)\n",
        "print(seq)\n",
        "print(pad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWWYblGelT6q"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFVoExU9D_Sb"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def parse_data(file):\n",
        "    for l in open(file,'r'):\n",
        "        yield json.loads(l)\n",
        "\n",
        "data = list(parse_data('/content/news-headlines-dataset-for-sarcasm-detection/Sarcasm_Headlines_Dataset.json'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnigRPTfEjG5"
      },
      "outputs": [],
      "source": [
        "sentances =[]\n",
        "lables=[]\n",
        "url=[]\n",
        "for item in data:\n",
        "  url.append(item['article_link'])\n",
        "  sentances.append(item['headline'])\n",
        "  lables.append(item['is_sarcastic'])\n",
        "print(lables)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWNrqZuVLVsN"
      },
      "outputs": [],
      "source": [
        "pad=tok(sentances)\n",
        "print(pad[12])\n",
        "pad.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inA2gZ6XZakj"
      },
      "source": [
        "# **Week2(Word Embeddings)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmgkx3qdZ76b"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1gBc4abqjwR"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNd5TVvCca2w"
      },
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "imdb ,info= tfds.load(\"imdb_reviews\" ,with_info=True ,as_supervised=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKWZpr2cdviT"
      },
      "outputs": [],
      "source": [
        "print(info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2BVqwx7Ld5dH"
      },
      "outputs": [],
      "source": [
        "print(imdb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6v0PccUeLvz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Get the train and test sets\n",
        "train_data, test_data = imdb['train'], imdb['test']\n",
        "\n",
        "# Initialize sentences and labels lists\n",
        "training_sentences = []\n",
        "training_labels = []\n",
        "\n",
        "testing_sentences = []\n",
        "testing_labels = []\n",
        "\n",
        "# Loop over all training examples and save the sentences and labels\n",
        "for s,l in train_data:\n",
        "  training_sentences.append(s.numpy().decode('utf8'))\n",
        "  training_labels.append(l.numpy())\n",
        "\n",
        "# Loop over all test examples and save the sentences and labels\n",
        "for s,l in test_data:\n",
        "  testing_sentences.append(s.numpy().decode('utf8'))\n",
        "  testing_labels.append(l.numpy())\n",
        "\n",
        "# Convert labels lists to numpy array\n",
        "training_labels_final = np.array(training_labels)\n",
        "testing_labels_final = np.array(testing_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2nwCqvFVY0J"
      },
      "source": [
        "word Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oSwqGWMIUg9o"
      },
      "outputs": [],
      "source": [
        "from keras import layers\n",
        "# Parameters\n",
        "\n",
        "vocab_size = 10000\n",
        "max_length = 120\n",
        "embedding_dim = 16\n",
        "trunc_type='post'\n",
        "oov_tok = \"<OOV>\"\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Initialize the Tokenizer class\n",
        "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
        "\n",
        "# Generate the word index dictionary for the training sentences\n",
        "tokenizer.fit_on_texts(training_sentences)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# Generate and pad the training sequences\n",
        "sequences = tokenizer.texts_to_sequences(training_sentences)\n",
        "padded = pad_sequences(sequences,maxlen=max_length, truncating=trunc_type)\n",
        "\n",
        "# Generate and pad the test sequences\n",
        "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
        "testing_padded = pad_sequences(testing_sequences,maxlen=max_length, truncating=trunc_type)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DiriAzg-4tlF"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Build the model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(6, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Setup the training parameters\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dl6k-ChO4yQC"
      },
      "outputs": [],
      "source": [
        "num_epochs = 10\n",
        "\n",
        "# Train the model\n",
        "model.fit(padded, training_labels_final, epochs=num_epochs, validation_data=(testing_padded, testing_labels_final))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5RlZ3_ZJ6xrv"
      },
      "outputs": [],
      "source": [
        "# Get the index-word dictionary\n",
        "reverse_word_index = tokenizer.index_word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOynu-np8Ee0"
      },
      "outputs": [],
      "source": [
        "# Get the embedding layer from the model (i.e. first layer)\n",
        "embedding_layer = model.layers[0]\n",
        "\n",
        "# Get the weights of the embedding layer\n",
        "embedding_weights = embedding_layer.get_weights()[0]\n",
        "\n",
        "# Print the shape. Expected is (vocab_size, embedding_dim)\n",
        "print(embedding_weights.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1GvhKJR7RhG"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "\n",
        "# Open writeable files\n",
        "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
        "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
        "\n",
        "# Initialize the loop. Start counting at `1` because `0` is just for the padding\n",
        "for word_num in range(1, vocab_size):\n",
        "\n",
        "  # Get the word associated at the current index\n",
        "  word_name = reverse_word_index[word_num]\n",
        "\n",
        "  # Get the embedding weights associated with the current index\n",
        "  word_embedding = embedding_weights[word_num]\n",
        "\n",
        "  # Write the word name\n",
        "  out_m.write(word_name + \"\\n\")\n",
        "\n",
        "  # Write the word embedding\n",
        "  out_v.write('\\t'.join([str(x) for x in word_embedding]) + \"\\n\")\n",
        "\n",
        "# Close the files\n",
        "out_v.close()\n",
        "out_m.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TubumwqB9noX"
      },
      "source": [
        "sarcasm dataset word embedding\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "opCl4q0t9Bk5"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "vocab_size =10000\n",
        "embededding_dim=16\n",
        "max_length=32\n",
        "training_size=20000\n",
        "def parse_data(file):\n",
        "    for l in open(file,'r'):\n",
        "        yield json.loads(l)\n",
        "\n",
        "data = list(parse_data('/content/news-headlines-dataset-for-sarcasm-detection/Sarcasm_Headlines_Dataset.json'))\n",
        "sentances =[]\n",
        "lables=[]\n",
        "for item in data:\n",
        "  sentances.append(item['headline'])\n",
        "  lables.append(item['is_sarcastic'])\n",
        "labels=np.array(lables)\n",
        "print(type(lables))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8MIDkvF-Y1X"
      },
      "outputs": [],
      "source": [
        "training_sentences=sentances[0:training_size]\n",
        "testing_sentences=sentances[training_size:]\n",
        "training_labels=lables[0:training_size]\n",
        "testing_labels=lables[training_size:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idAY1CyVD-Dy"
      },
      "outputs": [],
      "source": [
        "print(type(training_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "axvWt1ML_ItC"
      },
      "outputs": [],
      "source": [
        "tokenizer=Tokenizer(num_words=vocab_size,oov_token=\"<OOv>\")\n",
        "tokenizer.fit_on_texts(training_sentences)\n",
        "word_index=tokenizer.word_index\n",
        "training_sentences=tokenizer.texts_to_sequences(training_sentences)\n",
        "training_padded=pad_sequences(training_sentences,maxlen=max_length,truncating=\"post\",padding=\"post\")\n",
        "testing_sentences=tokenizer.texts_to_sequences(testing_sentences)\n",
        "testing_padded=pad_sequences(testing_sentences,maxlen=max_length,truncating=\"post\",padding=\"post\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPwG0cFSF2DC"
      },
      "outputs": [],
      "source": [
        "model=tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size,embedding_dim,input_length=max_length),\n",
        "    tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    tf.keras.layers.Dense(24,activation='relu'),\n",
        "    tf.keras.layers.Dense(1,activation=\"sigmoid\")\n",
        "])\n",
        "model.compile(loss='binary_crossentropy', metrics=['accuracy'],optimizer='adam')\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGbTUWANKa7j"
      },
      "outputs": [],
      "source": [
        "print(type(training_padded),type(training_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKMP9U62JCnY"
      },
      "outputs": [],
      "source": [
        "history =model.fit(training_padded,np.array(training_labels),epochs=30,\n",
        "                   validation_data=(testing_padded,np.array(testing_labels)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8Wgogfq-H9K"
      },
      "source": [
        "the accurcy afected by hyperparamitar like embedding_dimantion, number of word and number of epoches  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5pY7ACaMNS-"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plot_graph(history , string):\n",
        "  plt.figure(figsize=(5,5))\n",
        "  plt.plot(history.history[string])\n",
        "  plt.plot(history.history['val_'+string])\n",
        "  plt.xlabel('epoche')\n",
        "  plt.ylabel(string)\n",
        "  plt.legend([string,'val_'+string])\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zfp-5_4YQciz"
      },
      "outputs": [],
      "source": [
        "plot_graph(history,\"accuracy\")\n",
        "plot_graph(history,\"loss\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaDhW6KF-7VW"
      },
      "source": [
        "Pre_tokenized database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQvCRDnT_B8D"
      },
      "outputs": [],
      "source": [
        "imdb ,info =tfds.load(\"imdb_reviews/subwords8k\",with_info=True , as_supervised=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0klLaQMANPO"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = 10000\n",
        "BATCH_SIZE = 64\n",
        "train_data , test_data =imdb['train'],imdb['test']\n",
        "tokenizer_subwords =info.features['text'].encoder\n",
        "# Shuffle the training data\n",
        "train_dataset = train_data.shuffle(BUFFER_SIZE)\n",
        "\n",
        "# Batch and pad the datasets to the maximum length of the sequences\n",
        "train_dataset = train_dataset.padded_batch(BATCH_SIZE)\n",
        "test_dataset = test_data.padded_batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k20qs_M8AmMq"
      },
      "outputs": [],
      "source": [
        "print(tokenizer_subwords.subwords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XD9yeyDuBocx"
      },
      "outputs": [],
      "source": [
        "sample=\"hi Adan how are you\"\n",
        "tokenized_sample =tokenizer_subwords.encode(sample)\n",
        "print(\"sample after encoding {}\".format(tokenized_sample))\n",
        "sample_decoded =tokenizer_subwords.decode(tokenized_sample)\n",
        "print(\"sample after decode {}\".format(sample_decoded))\n",
        "for ts in tokenized_sample:\n",
        "  print('{}---->{}'.format(ts,tokenizer_subwords.decode([ts])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bAFf2sZWEAbH"
      },
      "outputs": [],
      "source": [
        "model=tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(tokenizer_subwords.vocab_size,64),\n",
        "    tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    tf.keras.layers.Dense(6,activation='relu'),\n",
        "    tf.keras.layers.Dense(1,activation=\"sigmoid\")\n",
        "])\n",
        "model.summary()\n",
        "model.compile(loss='binary_crossentropy', metrics=['accuracy'],optimizer='adam')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3VWVxwjwVZ39"
      },
      "outputs": [],
      "source": [
        "history = model.fit(train_dataset, epochs=10, validation_data=test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Ja-_3UVIQ3i"
      },
      "outputs": [],
      "source": [
        "plot_graph(history,\"accuracy\")\n",
        "plot_graph(history,\"loss\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUzBwOLD-hV6"
      },
      "source": [
        "# **Week3 (sequence models)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7f-z9-V-x3K"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6akP6fv5CGYn"
      },
      "outputs": [],
      "source": [
        "model=tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(tokenizer_subwords.vocab_size,64),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
        "    tf.keras.layers.Dense(64,activation='relu'),\n",
        "    tf.keras.layers.Dense(1,activation='sigmoid')\n",
        "])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IgDJ9f-QVL3u"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='binary_crossentropy', metrics=['accuracy'],optimizer='adam')\n",
        "history = model.fit(train_dataset, epochs=15, validation_data=test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6OYTtZu_i4nI"
      },
      "outputs": [],
      "source": [
        "plot_graph(history,\"loss\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VsPCTfiAnnuK"
      },
      "outputs": [],
      "source": [
        "plot_graph(history,'accuracy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CS3NG8p0arRY"
      },
      "source": [
        "# **Week4(Sequence models and literature)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXkXmU8gd93x"
      },
      "outputs": [],
      "source": [
        "tokenizer=Tokenizer()\n",
        "data=\"In the town of Athy one Jeremy Lanigan \\n Battered away til he hadnt a pound. \\nHis father died and made him a man again \\n Left him a farm and ten acres of ground. \\nHe gave a grand party for friends and relations \\nWho didnt forget him when come to the wall, \\nAnd if youll but listen Ill make your eyes glisten \\nOf the rows and the ructions of Lanigans Ball. \\nMyself to be sure got free invitation, \\nFor all the nice girls and boys I might ask, \\nAnd just in a minute both friends and relations \\nWere dancing round merry as bees round a cask. \\nJudy ODaly, that nice little milliner, \\nShe tipped me a wink for to give her a call, \\nAnd I soon arrived with Peggy McGilligan \\nJust in time for Lanigans Ball. \\nThere were lashings of punch and wine for the ladies, \\nPotatoes and cakes; there was bacon and tea, \\nThere were the Nolans, Dolans, OGradys \\nCourting the girls and dancing away. \\nSongs they went round as plenty as water, \\nThe harp that once sounded in Taras old hall,\\nSweet Nelly Gray and The Rat Catchers Daughter,\\nAll singing together at Lanigans Ball. \\nThey were doing all kinds of nonsensical polkas \\nAll round the room in a whirligig. \\nJulia and I, we banished their nonsense \\nAnd tipped them the twist of a reel and a jig. \\nAch mavrone, how the girls got all mad at me \\nDanced til youd think the ceiling would fall. \\nFor I spent three weeks at Brooks Academy \\nLearning new steps for Lanigans Ball. \\nThree long weeks I spent up in Dublin, \\nThree long weeks to learn nothing at all,\\n Three long weeks I spent up in Dublin, \\nLearning new steps for Lanigans Ball. \\nShe stepped out and I stepped in again, \\nI stepped out and she stepped in again, \\nShe stepped out and I stepped in again, \\nLearning new steps for Lanigans Ball. \\nBoys were all merry and the girls they were hearty \\nAnd danced all around in couples and groups, \\nTil an accident happened, young Terrance McCarthy \\nPut his right leg through miss Finnertys hoops. \\nPoor creature fainted and cried Meelia murther, \\nCalled for her brothers and gathered them all. \\nCarmody swore that hed go no further \\nTil he had satisfaction at Lanigans Ball. \\nIn the midst of the row miss Kerrigan fainted, \\nHer cheeks at the same time as red as a rose. \\nSome of the lads declared she was painted, \\nShe took a small drop too much, I suppose. \\nHer sweetheart, Ned Morgan, so powerful and able, \\nWhen he saw his fair colleen stretched out by the wall, \\nTore the left leg from under the table \\nAnd smashed all the Chaneys at Lanigans Ball. \\nBoys, oh boys, twas then there were runctions. \\nMyself got a lick from big Phelim McHugh. \\nI soon replied to his introduction \\nAnd kicked up a terrible hullabaloo. \\nOld Casey, the piper, was near being strangled. \\nThey squeezed up his pipes, bellows, chanters and all. \\nThe girls, in their ribbons, they got all entangled \\nAnd that put an end to Lanigans Ball.\"\n",
        "corpus=data.lower().split('\\n')\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "total_word=len(tokenizer.word_index)+1\n",
        "print(total_word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LzGeZnHZfrv9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "input_sentences=[]\n",
        "for line in corpus:\n",
        "    token_list=tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1,len(token_list)):\n",
        "      n_gram=token_list[:i+1]\n",
        "      input_sentences.append(n_gram)\n",
        "max_len_sentence = max([len(x) for x in input_sentences])\n",
        "input_sentences = np.array(pad_sequences(input_sentences, maxlen=max_len_sentence, padding='pre'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDc9g3fkjONr"
      },
      "outputs": [],
      "source": [
        "xs = input_sentences[:, :-1]\n",
        "labels = input_sentences[:, -1]\n",
        "ys=tf.keras.utils.to_categorical(labels,num_classes=total_word)\n",
        "print(xs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4J18VZfCIyo"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Embedding , LSTM , Dense , Bidirectional\n",
        "model=tf.keras.Sequential([\n",
        "    Embedding(total_word,64,input_length=max_len_sentence-1),\n",
        "    Bidirectional(LSTM(40)),\n",
        "    Dense(total_word , activation=\"softmax\")]\n",
        ")\n",
        "model.compile(optimizer=\"adam\", metrics=['accuracy'], loss=\"categorical_crossentropy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9tgnIRmENzV",
        "outputId": "d3aadae9-f526-4acc-a437-556eed6d13b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "15/15 [==============================] - 7s 116ms/step - loss: 5.5671 - accuracy: 0.0243\n",
            "Epoch 2/500\n",
            "15/15 [==============================] - 1s 37ms/step - loss: 5.5249 - accuracy: 0.0574\n",
            "Epoch 3/500\n",
            "15/15 [==============================] - 0s 34ms/step - loss: 5.3462 - accuracy: 0.0508\n",
            "Epoch 4/500\n",
            "15/15 [==============================] - 1s 41ms/step - loss: 5.1396 - accuracy: 0.0442\n",
            "Epoch 5/500\n",
            "15/15 [==============================] - 0s 20ms/step - loss: 5.0606 - accuracy: 0.0375\n",
            "Epoch 6/500\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 5.0305 - accuracy: 0.0419\n",
            "Epoch 7/500\n",
            "15/15 [==============================] - 0s 21ms/step - loss: 5.0047 - accuracy: 0.0464\n",
            "Epoch 8/500\n",
            "15/15 [==============================] - 0s 34ms/step - loss: 4.9601 - accuracy: 0.0508\n",
            "Epoch 9/500\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 4.9156 - accuracy: 0.0486\n",
            "Epoch 10/500\n",
            "15/15 [==============================] - 1s 42ms/step - loss: 4.8606 - accuracy: 0.0508\n",
            "Epoch 11/500\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 4.8017 - accuracy: 0.0640\n",
            "Epoch 12/500\n",
            "15/15 [==============================] - 0s 22ms/step - loss: 4.7400 - accuracy: 0.0596\n",
            "Epoch 13/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 4.6697 - accuracy: 0.0508\n",
            "Epoch 14/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 4.5941 - accuracy: 0.0530\n",
            "Epoch 15/500\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 4.5306 - accuracy: 0.0662\n",
            "Epoch 16/500\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 4.4868 - accuracy: 0.0684\n",
            "Epoch 17/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 4.4278 - accuracy: 0.0949\n",
            "Epoch 18/500\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 4.3716 - accuracy: 0.1060\n",
            "Epoch 19/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 4.3191 - accuracy: 0.0993\n",
            "Epoch 20/500\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 4.2681 - accuracy: 0.1104\n",
            "Epoch 21/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 4.2027 - accuracy: 0.1038\n",
            "Epoch 22/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 4.1373 - accuracy: 0.1214\n",
            "Epoch 23/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 4.0756 - accuracy: 0.1148\n",
            "Epoch 24/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 4.0345 - accuracy: 0.1258\n",
            "Epoch 25/500\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 3.9731 - accuracy: 0.1457\n",
            "Epoch 26/500\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 3.9159 - accuracy: 0.1766\n",
            "Epoch 27/500\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 3.8554 - accuracy: 0.1810\n",
            "Epoch 28/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 3.7769 - accuracy: 0.1810\n",
            "Epoch 29/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 3.7191 - accuracy: 0.2053\n",
            "Epoch 30/500\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 3.6503 - accuracy: 0.2141\n",
            "Epoch 31/500\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 3.5829 - accuracy: 0.2163\n",
            "Epoch 32/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 3.5304 - accuracy: 0.2208\n",
            "Epoch 33/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 3.4641 - accuracy: 0.2252\n",
            "Epoch 34/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 3.4183 - accuracy: 0.2362\n",
            "Epoch 35/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 3.3415 - accuracy: 0.2428\n",
            "Epoch 36/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 3.2850 - accuracy: 0.2539\n",
            "Epoch 37/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 3.2366 - accuracy: 0.2517\n",
            "Epoch 38/500\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 3.2090 - accuracy: 0.2914\n",
            "Epoch 39/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 3.1945 - accuracy: 0.2715\n",
            "Epoch 40/500\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 3.1028 - accuracy: 0.2958\n",
            "Epoch 41/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 3.0245 - accuracy: 0.3355\n",
            "Epoch 42/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 2.9737 - accuracy: 0.3311\n",
            "Epoch 43/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 2.9177 - accuracy: 0.3267\n",
            "Epoch 44/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 2.8561 - accuracy: 0.3620\n",
            "Epoch 45/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 2.8044 - accuracy: 0.3863\n",
            "Epoch 46/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 2.7616 - accuracy: 0.3974\n",
            "Epoch 47/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 2.7286 - accuracy: 0.4040\n",
            "Epoch 48/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 2.7095 - accuracy: 0.4106\n",
            "Epoch 49/500\n",
            "15/15 [==============================] - 0s 22ms/step - loss: 2.6329 - accuracy: 0.4371\n",
            "Epoch 50/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 2.5886 - accuracy: 0.4437\n",
            "Epoch 51/500\n",
            "15/15 [==============================] - 0s 20ms/step - loss: 2.5531 - accuracy: 0.4525\n",
            "Epoch 52/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 2.4933 - accuracy: 0.4967\n",
            "Epoch 53/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 2.4490 - accuracy: 0.4989\n",
            "Epoch 54/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 2.4107 - accuracy: 0.5143\n",
            "Epoch 55/500\n",
            "15/15 [==============================] - 0s 21ms/step - loss: 2.3576 - accuracy: 0.5188\n",
            "Epoch 56/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 2.3167 - accuracy: 0.5364\n",
            "Epoch 57/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 2.2871 - accuracy: 0.5563\n",
            "Epoch 58/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 2.2415 - accuracy: 0.5607\n",
            "Epoch 59/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 2.2217 - accuracy: 0.5695\n",
            "Epoch 60/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 2.1675 - accuracy: 0.5673\n",
            "Epoch 61/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 2.1373 - accuracy: 0.5740\n",
            "Epoch 62/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 2.1122 - accuracy: 0.5806\n",
            "Epoch 63/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 2.0584 - accuracy: 0.5960\n",
            "Epoch 64/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 2.0337 - accuracy: 0.6093\n",
            "Epoch 65/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 2.0116 - accuracy: 0.6071\n",
            "Epoch 66/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 1.9965 - accuracy: 0.5982\n",
            "Epoch 67/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.9517 - accuracy: 0.6181\n",
            "Epoch 68/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 1.9120 - accuracy: 0.6313\n",
            "Epoch 69/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.8560 - accuracy: 0.6512\n",
            "Epoch 70/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.8310 - accuracy: 0.6645\n",
            "Epoch 71/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.7884 - accuracy: 0.6821\n",
            "Epoch 72/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.7595 - accuracy: 0.6821\n",
            "Epoch 73/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 1.7208 - accuracy: 0.7042\n",
            "Epoch 74/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.6909 - accuracy: 0.7020\n",
            "Epoch 75/500\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 1.6667 - accuracy: 0.7174\n",
            "Epoch 76/500\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 1.6321 - accuracy: 0.7285\n",
            "Epoch 77/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 1.6008 - accuracy: 0.7307\n",
            "Epoch 78/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 1.5765 - accuracy: 0.7373\n",
            "Epoch 79/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 1.5416 - accuracy: 0.7550\n",
            "Epoch 80/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 1.5197 - accuracy: 0.7594\n",
            "Epoch 81/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 1.5045 - accuracy: 0.7638\n",
            "Epoch 82/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.4667 - accuracy: 0.7748\n",
            "Epoch 83/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.4343 - accuracy: 0.7748\n",
            "Epoch 84/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 1.4070 - accuracy: 0.7859\n",
            "Epoch 85/500\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 1.3825 - accuracy: 0.7815\n",
            "Epoch 86/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.3597 - accuracy: 0.7947\n",
            "Epoch 87/500\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 1.3388 - accuracy: 0.8102\n",
            "Epoch 88/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 1.3053 - accuracy: 0.8124\n",
            "Epoch 89/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.2836 - accuracy: 0.8102\n",
            "Epoch 90/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.2651 - accuracy: 0.8234\n",
            "Epoch 91/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.2430 - accuracy: 0.8146\n",
            "Epoch 92/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.2494 - accuracy: 0.8234\n",
            "Epoch 93/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 1.2249 - accuracy: 0.8168\n",
            "Epoch 94/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 1.2172 - accuracy: 0.8102\n",
            "Epoch 95/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.1942 - accuracy: 0.8256\n",
            "Epoch 96/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.2043 - accuracy: 0.8013\n",
            "Epoch 97/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 1.1854 - accuracy: 0.8212\n",
            "Epoch 98/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 1.1842 - accuracy: 0.7969\n",
            "Epoch 99/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.1694 - accuracy: 0.8102\n",
            "Epoch 100/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.1124 - accuracy: 0.8344\n",
            "Epoch 101/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.0770 - accuracy: 0.8455\n",
            "Epoch 102/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 1.0461 - accuracy: 0.8499\n",
            "Epoch 103/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 1.0121 - accuracy: 0.8587\n",
            "Epoch 104/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.9948 - accuracy: 0.8698\n",
            "Epoch 105/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.9717 - accuracy: 0.8698\n",
            "Epoch 106/500\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.9524 - accuracy: 0.8698\n",
            "Epoch 107/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.9339 - accuracy: 0.8786\n",
            "Epoch 108/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.9141 - accuracy: 0.8742\n",
            "Epoch 109/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.9008 - accuracy: 0.8786\n",
            "Epoch 110/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.8864 - accuracy: 0.8940\n",
            "Epoch 111/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.8769 - accuracy: 0.8698\n",
            "Epoch 112/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.8742 - accuracy: 0.8786\n",
            "Epoch 113/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.8510 - accuracy: 0.8786\n",
            "Epoch 114/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.8379 - accuracy: 0.8896\n",
            "Epoch 115/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.8222 - accuracy: 0.8940\n",
            "Epoch 116/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.8094 - accuracy: 0.9029\n",
            "Epoch 117/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.7865 - accuracy: 0.9073\n",
            "Epoch 118/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.7768 - accuracy: 0.9073\n",
            "Epoch 119/500\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.7604 - accuracy: 0.9073\n",
            "Epoch 120/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.7448 - accuracy: 0.9073\n",
            "Epoch 121/500\n",
            "15/15 [==============================] - 0s 22ms/step - loss: 0.8169 - accuracy: 0.8985\n",
            "Epoch 122/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.8268 - accuracy: 0.8896\n",
            "Epoch 123/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.8338 - accuracy: 0.8896\n",
            "Epoch 124/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.7921 - accuracy: 0.8830\n",
            "Epoch 125/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.7405 - accuracy: 0.9029\n",
            "Epoch 126/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.7148 - accuracy: 0.9007\n",
            "Epoch 127/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.6903 - accuracy: 0.9095\n",
            "Epoch 128/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.6679 - accuracy: 0.9205\n",
            "Epoch 129/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.6589 - accuracy: 0.9161\n",
            "Epoch 130/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.6445 - accuracy: 0.9249\n",
            "Epoch 131/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.6346 - accuracy: 0.9227\n",
            "Epoch 132/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.6175 - accuracy: 0.9227\n",
            "Epoch 133/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.6048 - accuracy: 0.9227\n",
            "Epoch 134/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.5932 - accuracy: 0.9294\n",
            "Epoch 135/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.5805 - accuracy: 0.9316\n",
            "Epoch 136/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.5731 - accuracy: 0.9272\n",
            "Epoch 137/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.5616 - accuracy: 0.9294\n",
            "Epoch 138/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.5524 - accuracy: 0.9294\n",
            "Epoch 139/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.5420 - accuracy: 0.9338\n",
            "Epoch 140/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.5349 - accuracy: 0.9316\n",
            "Epoch 141/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.5304 - accuracy: 0.9294\n",
            "Epoch 142/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.5226 - accuracy: 0.9316\n",
            "Epoch 143/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.5219 - accuracy: 0.9227\n",
            "Epoch 144/500\n",
            "15/15 [==============================] - 0s 20ms/step - loss: 0.5137 - accuracy: 0.9272\n",
            "Epoch 145/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.5016 - accuracy: 0.9272\n",
            "Epoch 146/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.4964 - accuracy: 0.9249\n",
            "Epoch 147/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.4838 - accuracy: 0.9382\n",
            "Epoch 148/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.4817 - accuracy: 0.9249\n",
            "Epoch 149/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.4723 - accuracy: 0.9382\n",
            "Epoch 150/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.4622 - accuracy: 0.9360\n",
            "Epoch 151/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.4556 - accuracy: 0.9426\n",
            "Epoch 152/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.4463 - accuracy: 0.9426\n",
            "Epoch 153/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.4391 - accuracy: 0.9360\n",
            "Epoch 154/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.4320 - accuracy: 0.9448\n",
            "Epoch 155/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.4270 - accuracy: 0.9404\n",
            "Epoch 156/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.4207 - accuracy: 0.9404\n",
            "Epoch 157/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.4164 - accuracy: 0.9404\n",
            "Epoch 158/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.4091 - accuracy: 0.9448\n",
            "Epoch 159/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.4025 - accuracy: 0.9470\n",
            "Epoch 160/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.4032 - accuracy: 0.9448\n",
            "Epoch 161/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.3906 - accuracy: 0.9448\n",
            "Epoch 162/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.3853 - accuracy: 0.9404\n",
            "Epoch 163/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.3792 - accuracy: 0.9470\n",
            "Epoch 164/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.3770 - accuracy: 0.9448\n",
            "Epoch 165/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.3746 - accuracy: 0.9492\n",
            "Epoch 166/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.3670 - accuracy: 0.9470\n",
            "Epoch 167/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.3611 - accuracy: 0.9448\n",
            "Epoch 168/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.3600 - accuracy: 0.9492\n",
            "Epoch 169/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.3588 - accuracy: 0.9426\n",
            "Epoch 170/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.3481 - accuracy: 0.9448\n",
            "Epoch 171/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.3430 - accuracy: 0.9492\n",
            "Epoch 172/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.3382 - accuracy: 0.9404\n",
            "Epoch 173/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.3335 - accuracy: 0.9492\n",
            "Epoch 174/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.3340 - accuracy: 0.9426\n",
            "Epoch 175/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.3360 - accuracy: 0.9492\n",
            "Epoch 176/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.3311 - accuracy: 0.9426\n",
            "Epoch 177/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.3228 - accuracy: 0.9448\n",
            "Epoch 178/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.3161 - accuracy: 0.9470\n",
            "Epoch 179/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.3135 - accuracy: 0.9426\n",
            "Epoch 180/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.3144 - accuracy: 0.9470\n",
            "Epoch 181/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.3140 - accuracy: 0.9470\n",
            "Epoch 182/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.3069 - accuracy: 0.9470\n",
            "Epoch 183/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.3039 - accuracy: 0.9426\n",
            "Epoch 184/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.3203 - accuracy: 0.9382\n",
            "Epoch 185/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.3153 - accuracy: 0.9404\n",
            "Epoch 186/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.2989 - accuracy: 0.9448\n",
            "Epoch 187/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.2944 - accuracy: 0.9426\n",
            "Epoch 188/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2866 - accuracy: 0.9514\n",
            "Epoch 189/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.2826 - accuracy: 0.9470\n",
            "Epoch 190/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.2777 - accuracy: 0.9514\n",
            "Epoch 191/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.2729 - accuracy: 0.9492\n",
            "Epoch 192/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.2846 - accuracy: 0.9426\n",
            "Epoch 193/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.2755 - accuracy: 0.9448\n",
            "Epoch 194/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.2963 - accuracy: 0.9470\n",
            "Epoch 195/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.3348 - accuracy: 0.9382\n",
            "Epoch 196/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.3502 - accuracy: 0.9161\n",
            "Epoch 197/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.3340 - accuracy: 0.9338\n",
            "Epoch 198/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.3056 - accuracy: 0.9338\n",
            "Epoch 199/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.2858 - accuracy: 0.9404\n",
            "Epoch 200/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.2705 - accuracy: 0.9426\n",
            "Epoch 201/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.2631 - accuracy: 0.9470\n",
            "Epoch 202/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.2565 - accuracy: 0.9404\n",
            "Epoch 203/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.2498 - accuracy: 0.9448\n",
            "Epoch 204/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.2525 - accuracy: 0.9404\n",
            "Epoch 205/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.2498 - accuracy: 0.9448\n",
            "Epoch 206/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.2453 - accuracy: 0.9492\n",
            "Epoch 207/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2462 - accuracy: 0.9426\n",
            "Epoch 208/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.2424 - accuracy: 0.9448\n",
            "Epoch 209/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.2366 - accuracy: 0.9492\n",
            "Epoch 210/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.2319 - accuracy: 0.9470\n",
            "Epoch 211/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.2305 - accuracy: 0.9448\n",
            "Epoch 212/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.2288 - accuracy: 0.9492\n",
            "Epoch 213/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2261 - accuracy: 0.9470\n",
            "Epoch 214/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.2223 - accuracy: 0.9492\n",
            "Epoch 215/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.2185 - accuracy: 0.9426\n",
            "Epoch 216/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.2168 - accuracy: 0.9448\n",
            "Epoch 217/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.2179 - accuracy: 0.9448\n",
            "Epoch 218/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.2154 - accuracy: 0.9536\n",
            "Epoch 219/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.2118 - accuracy: 0.9448\n",
            "Epoch 220/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.2105 - accuracy: 0.9492\n",
            "Epoch 221/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2075 - accuracy: 0.9536\n",
            "Epoch 222/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.2062 - accuracy: 0.9536\n",
            "Epoch 223/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.2053 - accuracy: 0.9492\n",
            "Epoch 224/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.2091 - accuracy: 0.9492\n",
            "Epoch 225/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.2055 - accuracy: 0.9492\n",
            "Epoch 226/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.2019 - accuracy: 0.9470\n",
            "Epoch 227/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1976 - accuracy: 0.9470\n",
            "Epoch 228/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1969 - accuracy: 0.9514\n",
            "Epoch 229/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.1945 - accuracy: 0.9470\n",
            "Epoch 230/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1925 - accuracy: 0.9492\n",
            "Epoch 231/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1908 - accuracy: 0.9470\n",
            "Epoch 232/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1887 - accuracy: 0.9492\n",
            "Epoch 233/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1866 - accuracy: 0.9470\n",
            "Epoch 234/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.1862 - accuracy: 0.9492\n",
            "Epoch 235/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.1891 - accuracy: 0.9448\n",
            "Epoch 236/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1995 - accuracy: 0.9492\n",
            "Epoch 237/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1961 - accuracy: 0.9514\n",
            "Epoch 238/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1899 - accuracy: 0.9492\n",
            "Epoch 239/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1897 - accuracy: 0.9470\n",
            "Epoch 240/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.1877 - accuracy: 0.9470\n",
            "Epoch 241/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1878 - accuracy: 0.9470\n",
            "Epoch 242/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.2459 - accuracy: 0.9338\n",
            "Epoch 243/500\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.2668 - accuracy: 0.9294\n",
            "Epoch 244/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.2726 - accuracy: 0.9139\n",
            "Epoch 245/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2437 - accuracy: 0.9294\n",
            "Epoch 246/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.2304 - accuracy: 0.9404\n",
            "Epoch 247/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.2073 - accuracy: 0.9470\n",
            "Epoch 248/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.2211 - accuracy: 0.9426\n",
            "Epoch 249/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.2090 - accuracy: 0.9448\n",
            "Epoch 250/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.1932 - accuracy: 0.9448\n",
            "Epoch 251/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.2054 - accuracy: 0.9448\n",
            "Epoch 252/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.1933 - accuracy: 0.9448\n",
            "Epoch 253/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1863 - accuracy: 0.9448\n",
            "Epoch 254/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.1774 - accuracy: 0.9492\n",
            "Epoch 255/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.1765 - accuracy: 0.9470\n",
            "Epoch 256/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.1704 - accuracy: 0.9492\n",
            "Epoch 257/500\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.1653 - accuracy: 0.9536\n",
            "Epoch 258/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.1710 - accuracy: 0.9492\n",
            "Epoch 259/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.1665 - accuracy: 0.9492\n",
            "Epoch 260/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.1637 - accuracy: 0.9470\n",
            "Epoch 261/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.1594 - accuracy: 0.9492\n",
            "Epoch 262/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.1598 - accuracy: 0.9492\n",
            "Epoch 263/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1580 - accuracy: 0.9492\n",
            "Epoch 264/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.1605 - accuracy: 0.9536\n",
            "Epoch 265/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.1581 - accuracy: 0.9470\n",
            "Epoch 266/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1551 - accuracy: 0.9492\n",
            "Epoch 267/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.1521 - accuracy: 0.9492\n",
            "Epoch 268/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.1519 - accuracy: 0.9514\n",
            "Epoch 269/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.1521 - accuracy: 0.9426\n",
            "Epoch 270/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.1501 - accuracy: 0.9448\n",
            "Epoch 271/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.1487 - accuracy: 0.9426\n",
            "Epoch 272/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.1477 - accuracy: 0.9426\n",
            "Epoch 273/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.1458 - accuracy: 0.9470\n",
            "Epoch 274/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1451 - accuracy: 0.9448\n",
            "Epoch 275/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1454 - accuracy: 0.9536\n",
            "Epoch 276/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1455 - accuracy: 0.9426\n",
            "Epoch 277/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.1448 - accuracy: 0.9426\n",
            "Epoch 278/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.1435 - accuracy: 0.9426\n",
            "Epoch 279/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1422 - accuracy: 0.9448\n",
            "Epoch 280/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1419 - accuracy: 0.9470\n",
            "Epoch 281/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1410 - accuracy: 0.9492\n",
            "Epoch 282/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1403 - accuracy: 0.9448\n",
            "Epoch 283/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1396 - accuracy: 0.9470\n",
            "Epoch 284/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1416 - accuracy: 0.9448\n",
            "Epoch 285/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1406 - accuracy: 0.9492\n",
            "Epoch 286/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.1407 - accuracy: 0.9448\n",
            "Epoch 287/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.1386 - accuracy: 0.9492\n",
            "Epoch 288/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.1380 - accuracy: 0.9404\n",
            "Epoch 289/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1373 - accuracy: 0.9470\n",
            "Epoch 290/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1359 - accuracy: 0.9514\n",
            "Epoch 291/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1353 - accuracy: 0.9492\n",
            "Epoch 292/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1354 - accuracy: 0.9470\n",
            "Epoch 293/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.1342 - accuracy: 0.9448\n",
            "Epoch 294/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1341 - accuracy: 0.9448\n",
            "Epoch 295/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1338 - accuracy: 0.9426\n",
            "Epoch 296/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1326 - accuracy: 0.9448\n",
            "Epoch 297/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.1323 - accuracy: 0.9404\n",
            "Epoch 298/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.1313 - accuracy: 0.9470\n",
            "Epoch 299/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1316 - accuracy: 0.9470\n",
            "Epoch 300/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1303 - accuracy: 0.9448\n",
            "Epoch 301/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1298 - accuracy: 0.9448\n",
            "Epoch 302/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1301 - accuracy: 0.9448\n",
            "Epoch 303/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1287 - accuracy: 0.9470\n",
            "Epoch 304/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1299 - accuracy: 0.9470\n",
            "Epoch 305/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1289 - accuracy: 0.9470\n",
            "Epoch 306/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1291 - accuracy: 0.9404\n",
            "Epoch 307/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1289 - accuracy: 0.9536\n",
            "Epoch 308/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1295 - accuracy: 0.9448\n",
            "Epoch 309/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.1289 - accuracy: 0.9470\n",
            "Epoch 310/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1274 - accuracy: 0.9492\n",
            "Epoch 311/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1264 - accuracy: 0.9536\n",
            "Epoch 312/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1257 - accuracy: 0.9470\n",
            "Epoch 313/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1258 - accuracy: 0.9426\n",
            "Epoch 314/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1247 - accuracy: 0.9470\n",
            "Epoch 315/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.1253 - accuracy: 0.9426\n",
            "Epoch 316/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1238 - accuracy: 0.9470\n",
            "Epoch 317/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1252 - accuracy: 0.9514\n",
            "Epoch 318/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.1242 - accuracy: 0.9514\n",
            "Epoch 319/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1269 - accuracy: 0.9514\n",
            "Epoch 320/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1272 - accuracy: 0.9470\n",
            "Epoch 321/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1308 - accuracy: 0.9514\n",
            "Epoch 322/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.1268 - accuracy: 0.9492\n",
            "Epoch 323/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1242 - accuracy: 0.9448\n",
            "Epoch 324/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1268 - accuracy: 0.9448\n",
            "Epoch 325/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1379 - accuracy: 0.9448\n",
            "Epoch 326/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1425 - accuracy: 0.9448\n",
            "Epoch 327/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1499 - accuracy: 0.9382\n",
            "Epoch 328/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1620 - accuracy: 0.9448\n",
            "Epoch 329/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1866 - accuracy: 0.9338\n",
            "Epoch 330/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.1737 - accuracy: 0.9360\n",
            "Epoch 331/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1601 - accuracy: 0.9404\n",
            "Epoch 332/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.1565 - accuracy: 0.9448\n",
            "Epoch 333/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1449 - accuracy: 0.9470\n",
            "Epoch 334/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1473 - accuracy: 0.9426\n",
            "Epoch 335/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1529 - accuracy: 0.9404\n",
            "Epoch 336/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1379 - accuracy: 0.9514\n",
            "Epoch 337/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1329 - accuracy: 0.9426\n",
            "Epoch 338/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1239 - accuracy: 0.9448\n",
            "Epoch 339/500\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.1219 - accuracy: 0.9492\n",
            "Epoch 340/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1219 - accuracy: 0.9492\n",
            "Epoch 341/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1225 - accuracy: 0.9470\n",
            "Epoch 342/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1199 - accuracy: 0.9448\n",
            "Epoch 343/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1182 - accuracy: 0.9448\n",
            "Epoch 344/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1175 - accuracy: 0.9448\n",
            "Epoch 345/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1171 - accuracy: 0.9404\n",
            "Epoch 346/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1156 - accuracy: 0.9448\n",
            "Epoch 347/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1161 - accuracy: 0.9426\n",
            "Epoch 348/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1157 - accuracy: 0.9470\n",
            "Epoch 349/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1155 - accuracy: 0.9426\n",
            "Epoch 350/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.1153 - accuracy: 0.9470\n",
            "Epoch 351/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.1150 - accuracy: 0.9492\n",
            "Epoch 352/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.1139 - accuracy: 0.9426\n",
            "Epoch 353/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1157 - accuracy: 0.9448\n",
            "Epoch 354/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.1137 - accuracy: 0.9426\n",
            "Epoch 355/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.1132 - accuracy: 0.9514\n",
            "Epoch 356/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1129 - accuracy: 0.9448\n",
            "Epoch 357/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.1129 - accuracy: 0.9448\n",
            "Epoch 358/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.1110 - accuracy: 0.9492\n",
            "Epoch 359/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1129 - accuracy: 0.9426\n",
            "Epoch 360/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1135 - accuracy: 0.9448\n",
            "Epoch 361/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.1113 - accuracy: 0.9404\n",
            "Epoch 362/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.1115 - accuracy: 0.9382\n",
            "Epoch 363/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1111 - accuracy: 0.9448\n",
            "Epoch 364/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.1133 - accuracy: 0.9448\n",
            "Epoch 365/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1151 - accuracy: 0.9492\n",
            "Epoch 366/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.1122 - accuracy: 0.9448\n",
            "Epoch 367/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.1118 - accuracy: 0.9404\n",
            "Epoch 368/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.1119 - accuracy: 0.9514\n",
            "Epoch 369/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.1123 - accuracy: 0.9448\n",
            "Epoch 370/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.1110 - accuracy: 0.9426\n",
            "Epoch 371/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1097 - accuracy: 0.9426\n",
            "Epoch 372/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1102 - accuracy: 0.9448\n",
            "Epoch 373/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.1103 - accuracy: 0.9426\n",
            "Epoch 374/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.1084 - accuracy: 0.9492\n",
            "Epoch 375/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.1118 - accuracy: 0.9448\n",
            "Epoch 376/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1113 - accuracy: 0.9448\n",
            "Epoch 377/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.1099 - accuracy: 0.9448\n",
            "Epoch 378/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.1078 - accuracy: 0.9492\n",
            "Epoch 379/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.1083 - accuracy: 0.9404\n",
            "Epoch 380/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.1075 - accuracy: 0.9404\n",
            "Epoch 381/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1091 - accuracy: 0.9448\n",
            "Epoch 382/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1093 - accuracy: 0.9404\n",
            "Epoch 383/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1098 - accuracy: 0.9448\n",
            "Epoch 384/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1085 - accuracy: 0.9448\n",
            "Epoch 385/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1071 - accuracy: 0.9426\n",
            "Epoch 386/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1055 - accuracy: 0.9470\n",
            "Epoch 387/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1056 - accuracy: 0.9426\n",
            "Epoch 388/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1043 - accuracy: 0.9492\n",
            "Epoch 389/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.1048 - accuracy: 0.9448\n",
            "Epoch 390/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1067 - accuracy: 0.9448\n",
            "Epoch 391/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1065 - accuracy: 0.9426\n",
            "Epoch 392/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1065 - accuracy: 0.9382\n",
            "Epoch 393/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.1054 - accuracy: 0.9426\n",
            "Epoch 394/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1051 - accuracy: 0.9492\n",
            "Epoch 395/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1044 - accuracy: 0.9448\n",
            "Epoch 396/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.1046 - accuracy: 0.9470\n",
            "Epoch 397/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1063 - accuracy: 0.9448\n",
            "Epoch 398/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1042 - accuracy: 0.9426\n",
            "Epoch 399/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.1035 - accuracy: 0.9404\n",
            "Epoch 400/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1042 - accuracy: 0.9382\n",
            "Epoch 401/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1055 - accuracy: 0.9470\n",
            "Epoch 402/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1042 - accuracy: 0.9492\n",
            "Epoch 403/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1075 - accuracy: 0.9470\n",
            "Epoch 404/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1052 - accuracy: 0.9448\n",
            "Epoch 405/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1052 - accuracy: 0.9382\n",
            "Epoch 406/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1052 - accuracy: 0.9492\n",
            "Epoch 407/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1057 - accuracy: 0.9448\n",
            "Epoch 408/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1059 - accuracy: 0.9404\n",
            "Epoch 409/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1048 - accuracy: 0.9470\n",
            "Epoch 410/500\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.1045 - accuracy: 0.9382\n",
            "Epoch 411/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1032 - accuracy: 0.9492\n",
            "Epoch 412/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1041 - accuracy: 0.9470\n",
            "Epoch 413/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1094 - accuracy: 0.9448\n",
            "Epoch 414/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.1042 - accuracy: 0.9426\n",
            "Epoch 415/500\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.1017 - accuracy: 0.9470\n",
            "Epoch 416/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1028 - accuracy: 0.9404\n",
            "Epoch 417/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1029 - accuracy: 0.9404\n",
            "Epoch 418/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1041 - accuracy: 0.9514\n",
            "Epoch 419/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.1020 - accuracy: 0.9470\n",
            "Epoch 420/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1021 - accuracy: 0.9492\n",
            "Epoch 421/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1012 - accuracy: 0.9404\n",
            "Epoch 422/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1017 - accuracy: 0.9448\n",
            "Epoch 423/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1014 - accuracy: 0.9470\n",
            "Epoch 424/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1000 - accuracy: 0.9514\n",
            "Epoch 425/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1008 - accuracy: 0.9382\n",
            "Epoch 426/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1002 - accuracy: 0.9426\n",
            "Epoch 427/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1011 - accuracy: 0.9382\n",
            "Epoch 428/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.0997 - accuracy: 0.9448\n",
            "Epoch 429/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.0996 - accuracy: 0.9426\n",
            "Epoch 430/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1008 - accuracy: 0.9448\n",
            "Epoch 431/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1005 - accuracy: 0.9470\n",
            "Epoch 432/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1024 - accuracy: 0.9514\n",
            "Epoch 433/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.1022 - accuracy: 0.9448\n",
            "Epoch 434/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1040 - accuracy: 0.9470\n",
            "Epoch 435/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1056 - accuracy: 0.9492\n",
            "Epoch 436/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1092 - accuracy: 0.9448\n",
            "Epoch 437/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1037 - accuracy: 0.9448\n",
            "Epoch 438/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1013 - accuracy: 0.9448\n",
            "Epoch 439/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1028 - accuracy: 0.9448\n",
            "Epoch 440/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1051 - accuracy: 0.9448\n",
            "Epoch 441/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1063 - accuracy: 0.9492\n",
            "Epoch 442/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1068 - accuracy: 0.9514\n",
            "Epoch 443/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1044 - accuracy: 0.9470\n",
            "Epoch 444/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1021 - accuracy: 0.9470\n",
            "Epoch 445/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1055 - accuracy: 0.9426\n",
            "Epoch 446/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1018 - accuracy: 0.9514\n",
            "Epoch 447/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1006 - accuracy: 0.9492\n",
            "Epoch 448/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.1001 - accuracy: 0.9426\n",
            "Epoch 449/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1004 - accuracy: 0.9426\n",
            "Epoch 450/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1014 - accuracy: 0.9382\n",
            "Epoch 451/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1010 - accuracy: 0.9382\n",
            "Epoch 452/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.0991 - accuracy: 0.9426\n",
            "Epoch 453/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1002 - accuracy: 0.9426\n",
            "Epoch 454/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1002 - accuracy: 0.9492\n",
            "Epoch 455/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.0993 - accuracy: 0.9492\n",
            "Epoch 456/500\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.0994 - accuracy: 0.9470\n",
            "Epoch 457/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.0989 - accuracy: 0.9470\n",
            "Epoch 458/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.0984 - accuracy: 0.9448\n",
            "Epoch 459/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.0984 - accuracy: 0.9448\n",
            "Epoch 460/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.0992 - accuracy: 0.9470\n",
            "Epoch 461/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.0980 - accuracy: 0.9492\n",
            "Epoch 462/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.0983 - accuracy: 0.9470\n",
            "Epoch 463/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.0983 - accuracy: 0.9470\n",
            "Epoch 464/500\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.1004 - accuracy: 0.9404\n",
            "Epoch 465/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.0986 - accuracy: 0.9448\n",
            "Epoch 466/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.0974 - accuracy: 0.9492\n",
            "Epoch 467/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.0985 - accuracy: 0.9492\n",
            "Epoch 468/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1003 - accuracy: 0.9448\n",
            "Epoch 469/500\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.0998 - accuracy: 0.9470\n",
            "Epoch 470/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.0982 - accuracy: 0.9448\n",
            "Epoch 471/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.0953 - accuracy: 0.9404\n",
            "Epoch 472/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.1004 - accuracy: 0.9426\n",
            "Epoch 473/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.0983 - accuracy: 0.9492\n",
            "Epoch 474/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.0996 - accuracy: 0.9426\n",
            "Epoch 475/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.0987 - accuracy: 0.9448\n",
            "Epoch 476/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.0982 - accuracy: 0.9404\n",
            "Epoch 477/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.0980 - accuracy: 0.9514\n",
            "Epoch 478/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.0977 - accuracy: 0.9470\n",
            "Epoch 479/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.0963 - accuracy: 0.9448\n",
            "Epoch 480/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.0971 - accuracy: 0.9492\n",
            "Epoch 481/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.0979 - accuracy: 0.9470\n",
            "Epoch 482/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.0992 - accuracy: 0.9426\n",
            "Epoch 483/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.0975 - accuracy: 0.9470\n",
            "Epoch 484/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.0967 - accuracy: 0.9448\n",
            "Epoch 485/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.0978 - accuracy: 0.9514\n",
            "Epoch 486/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.0967 - accuracy: 0.9426\n",
            "Epoch 487/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.0965 - accuracy: 0.9426\n",
            "Epoch 488/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.0956 - accuracy: 0.9470\n",
            "Epoch 489/500\n",
            "15/15 [==============================] - 0s 20ms/step - loss: 0.0957 - accuracy: 0.9426\n",
            "Epoch 490/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.0949 - accuracy: 0.9492\n",
            "Epoch 491/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.0960 - accuracy: 0.9404\n",
            "Epoch 492/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.0959 - accuracy: 0.9448\n",
            "Epoch 493/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.0960 - accuracy: 0.9514\n",
            "Epoch 494/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.0965 - accuracy: 0.9426\n",
            "Epoch 495/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.0950 - accuracy: 0.9470\n",
            "Epoch 496/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.0950 - accuracy: 0.9492\n",
            "Epoch 497/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.0986 - accuracy: 0.9492\n",
            "Epoch 498/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.0979 - accuracy: 0.9470\n",
            "Epoch 499/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.0987 - accuracy: 0.9448\n",
            "Epoch 500/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.0982 - accuracy: 0.9448\n"
          ]
        }
      ],
      "source": [
        "history=model.fit(xs,ys,epochs=500,verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.xlabel('epoche')\n",
        "plt.ylabel('accuracy')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "WT6XRPwmRzwd",
        "outputId": "7b36aae6-f760-47ae-8c1c-1977b07277d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed_text=\"Laurence went to dublin\"\n",
        "token_list=tokenizer.texts_to_sequences([seed_text])[0]\n",
        "token_list=pad_sequences([token_list] ,maxlen=max_len_sentence-1 , padding='pre')\n",
        "prop=model.predict(token_list)\n",
        "result=np.argmax(prop ,axis=-1)[0]\n",
        "out_word=tokenizer.index_word[result]\n",
        "seed_text+=\" \"+ out_word\n",
        "print(seed_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZlqxDNwXqgN",
        "outputId": "8c0ce323-a8f4-4209-b0ae-9ddf5295dd09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 30ms/step\n",
            "Laurence went to dublin boys\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed_text=\"Laurence went to dublin\"\n",
        "next_word=10\n",
        "for _ in range(next_word):\n",
        "  token_list=tokenizer.texts_to_sequences([seed_text])[0]\n",
        "  token_list=pad_sequences([token_list] ,maxlen=max_len_sentence-1 , padding='pre')\n",
        "  prop=model.predict(token_list)\n",
        "  result=np.argmax(prop ,axis=-1)[0]\n",
        "  out_word=tokenizer.index_word[result]\n",
        "  seed_text+=\" \"+ out_word\n",
        "  print(seed_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-p-rCacbSMJ",
        "outputId": "4d52f191-7159-4428-ebdf-d7a62bf41c05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 61ms/step\n",
            "Laurence went to dublin boys\n",
            "1/1 [==============================] - 0s 107ms/step\n",
            "Laurence went to dublin boys the\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "Laurence went to dublin boys the ructions\n",
            "1/1 [==============================] - 0s 99ms/step\n",
            "Laurence went to dublin boys the ructions of\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "Laurence went to dublin boys the ructions of and\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "Laurence went to dublin boys the ructions of and girls\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "Laurence went to dublin boys the ructions of and girls were\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "Laurence went to dublin boys the ructions of and girls were had\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "Laurence went to dublin boys the ructions of and girls were had eyes\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "Laurence went to dublin boys the ructions of and girls were had eyes rose\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Poetry"
      ],
      "metadata": {
        "id": "qQAYGXQHgJI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data=open('/content/Laurences_generated_poetry.txt').read()\n",
        "tokenizer=Tokenizer()\n",
        "corpus=data.lower().split('\\n')\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "total_word=len(tokenizer.word_index)+1\n",
        "print(total_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6lQ0EHre_VA",
        "outputId": "ba087110-1165-4c57-928c-c80f81649d68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2690\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mzgnkg2gf93"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "input_sentences=[]\n",
        "for line in corpus:\n",
        "    token_list=tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1,len(token_list)):\n",
        "      n_gram=token_list[:i+1]\n",
        "      input_sentences.append(n_gram)\n",
        "max_len_sentence = max([len(x) for x in input_sentences])\n",
        "input_sentences = np.array(pad_sequences(input_sentences, maxlen=max_len_sentence, padding='pre'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uf_kt0zpgqLp",
        "outputId": "319de142-eb3e-442a-83aa-3a7157ef8f09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  0   0   0 ...   0   0  51]\n",
            " [  0   0   0 ...   0  51  12]\n",
            " [  0   0   0 ...  51  12  96]\n",
            " ...\n",
            " [  0   0   0 ...   0   0  47]\n",
            " [  0   0   0 ...   0  47 105]\n",
            " [  0   0   0 ...  47 105 138]]\n"
          ]
        }
      ],
      "source": [
        "xs = input_sentences[:, :-1]\n",
        "labels = input_sentences[:, -1]\n",
        "ys=tf.keras.utils.to_categorical(labels,num_classes=total_word)\n",
        "print(xs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Embedding , LSTM , Dense , Bidirectional\n",
        "from keras.optimizers import Adam\n",
        "model=tf.keras.Sequential([\n",
        "    Embedding(total_word,100,input_length=max_len_sentence-1),\n",
        "    Bidirectional(LSTM(150)),\n",
        "    Dense(total_word , activation=\"softmax\")]\n",
        ")\n",
        "adam=Adam(learning_rate=0.01)\n",
        "model.compile(optimizer=adam, metrics=['accuracy'], loss=\"categorical_crossentropy\")"
      ],
      "metadata": {
        "id": "gxI7lGjLgsQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history=model.fit(xs,ys,epochs=500,verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hGqGHAwiCc0",
        "outputId": "03dc484d-50d8-484e-dd1b-e5e497d895a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "377/377 [==============================] - 17s 36ms/step - loss: 6.6585 - accuracy: 0.0735\n",
            "Epoch 2/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 5.7441 - accuracy: 0.1126\n",
            "Epoch 3/500\n",
            "377/377 [==============================] - 5s 13ms/step - loss: 4.8718 - accuracy: 0.1687\n",
            "Epoch 4/500\n",
            "377/377 [==============================] - 4s 11ms/step - loss: 3.8919 - accuracy: 0.2482\n",
            "Epoch 5/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 3.0079 - accuracy: 0.3540\n",
            "Epoch 6/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 2.3156 - accuracy: 0.4774\n",
            "Epoch 7/500\n",
            "377/377 [==============================] - 6s 16ms/step - loss: 1.8385 - accuracy: 0.5668\n",
            "Epoch 8/500\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 1.4818 - accuracy: 0.6440\n",
            "Epoch 9/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 1.2662 - accuracy: 0.6972\n",
            "Epoch 10/500\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 1.1498 - accuracy: 0.7229\n",
            "Epoch 11/500\n",
            "377/377 [==============================] - 4s 11ms/step - loss: 1.0746 - accuracy: 0.7368\n",
            "Epoch 12/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 1.0267 - accuracy: 0.7489\n",
            "Epoch 13/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 1.0517 - accuracy: 0.7338\n",
            "Epoch 14/500\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 1.1022 - accuracy: 0.7232\n",
            "Epoch 15/500\n",
            "377/377 [==============================] - 4s 12ms/step - loss: 1.1958 - accuracy: 0.6943\n",
            "Epoch 16/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 1.2414 - accuracy: 0.6811\n",
            "Epoch 17/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 1.0695 - accuracy: 0.7259\n",
            "Epoch 18/500\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.9487 - accuracy: 0.7568\n",
            "Epoch 19/500\n",
            "377/377 [==============================] - 4s 12ms/step - loss: 0.8926 - accuracy: 0.7725\n",
            "Epoch 20/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.8504 - accuracy: 0.7842\n",
            "Epoch 21/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.8133 - accuracy: 0.7929\n",
            "Epoch 22/500\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.8425 - accuracy: 0.7866\n",
            "Epoch 23/500\n",
            "377/377 [==============================] - 4s 11ms/step - loss: 0.9244 - accuracy: 0.7616\n",
            "Epoch 24/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 1.1960 - accuracy: 0.6937\n",
            "Epoch 25/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 1.2751 - accuracy: 0.6738\n",
            "Epoch 26/500\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 1.1393 - accuracy: 0.7000\n",
            "Epoch 27/500\n",
            "377/377 [==============================] - 4s 11ms/step - loss: 0.9860 - accuracy: 0.7456\n",
            "Epoch 28/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.8827 - accuracy: 0.7690\n",
            "Epoch 29/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.8231 - accuracy: 0.7848\n",
            "Epoch 30/500\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.7957 - accuracy: 0.7925\n",
            "Epoch 31/500\n",
            "377/377 [==============================] - 4s 11ms/step - loss: 0.7711 - accuracy: 0.8001\n",
            "Epoch 32/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.8195 - accuracy: 0.7854\n",
            "Epoch 33/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.9511 - accuracy: 0.7494\n",
            "Epoch 34/500\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 1.1269 - accuracy: 0.7058\n",
            "Epoch 35/500\n",
            "377/377 [==============================] - 4s 11ms/step - loss: 1.0828 - accuracy: 0.7201\n",
            "Epoch 36/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 1.0513 - accuracy: 0.7230\n",
            "Epoch 37/500\n",
            "377/377 [==============================] - 4s 9ms/step - loss: 0.9748 - accuracy: 0.7439\n",
            "Epoch 38/500\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.8426 - accuracy: 0.7751\n",
            "Epoch 39/500\n",
            "377/377 [==============================] - 5s 12ms/step - loss: 0.7582 - accuracy: 0.8018\n",
            "Epoch 40/500\n",
            "377/377 [==============================] - 4s 9ms/step - loss: 0.7129 - accuracy: 0.8135\n",
            "Epoch 41/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.7074 - accuracy: 0.8155\n",
            "Epoch 42/500\n",
            "377/377 [==============================] - 4s 11ms/step - loss: 0.7854 - accuracy: 0.7966\n",
            "Epoch 43/500\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 1.1600 - accuracy: 0.7077\n",
            "Epoch 44/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 1.3032 - accuracy: 0.6691\n",
            "Epoch 45/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 1.1308 - accuracy: 0.7096\n",
            "Epoch 46/500\n",
            "377/377 [==============================] - 4s 11ms/step - loss: 0.9768 - accuracy: 0.7403\n",
            "Epoch 47/500\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.8289 - accuracy: 0.7774\n",
            "Epoch 48/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.7481 - accuracy: 0.8012\n",
            "Epoch 49/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.7010 - accuracy: 0.8173\n",
            "Epoch 50/500\n",
            "377/377 [==============================] - 6s 15ms/step - loss: 0.7225 - accuracy: 0.8059\n",
            "Epoch 51/500\n",
            "377/377 [==============================] - 5s 12ms/step - loss: 0.7767 - accuracy: 0.7955\n",
            "Epoch 52/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.8910 - accuracy: 0.7640\n",
            "Epoch 53/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.9894 - accuracy: 0.7368\n",
            "Epoch 54/500\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 1.0999 - accuracy: 0.7142\n",
            "Epoch 55/500\n",
            "377/377 [==============================] - 4s 11ms/step - loss: 1.1085 - accuracy: 0.7159\n",
            "Epoch 56/500\n",
            "377/377 [==============================] - 4s 9ms/step - loss: 0.9517 - accuracy: 0.7463\n",
            "Epoch 57/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.8357 - accuracy: 0.7772\n",
            "Epoch 58/500\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.7972 - accuracy: 0.7873\n",
            "Epoch 59/500\n",
            "377/377 [==============================] - 4s 11ms/step - loss: 0.7498 - accuracy: 0.7974\n",
            "Epoch 60/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.7753 - accuracy: 0.7975\n",
            "Epoch 61/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.7530 - accuracy: 0.8029\n",
            "Epoch 62/500\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.8122 - accuracy: 0.7899\n",
            "Epoch 63/500\n",
            "377/377 [==============================] - 5s 12ms/step - loss: 0.8759 - accuracy: 0.7735\n",
            "Epoch 64/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.9868 - accuracy: 0.7400\n",
            "Epoch 65/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 1.0456 - accuracy: 0.7267\n",
            "Epoch 66/500\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 1.0187 - accuracy: 0.7392\n",
            "Epoch 67/500\n",
            "377/377 [==============================] - 4s 11ms/step - loss: 0.8922 - accuracy: 0.7632\n",
            "Epoch 68/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.7952 - accuracy: 0.7882\n",
            "Epoch 69/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.7411 - accuracy: 0.8041\n",
            "Epoch 70/500\n",
            "377/377 [==============================] - 4s 11ms/step - loss: 0.7619 - accuracy: 0.8005\n",
            "Epoch 71/500\n",
            "377/377 [==============================] - 4s 11ms/step - loss: 0.8962 - accuracy: 0.7677\n",
            "Epoch 72/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 1.0565 - accuracy: 0.7274\n",
            "Epoch 73/500\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.9761 - accuracy: 0.7412\n",
            "Epoch 74/500\n",
            "377/377 [==============================] - 6s 17ms/step - loss: 0.9018 - accuracy: 0.7606\n",
            "Epoch 75/500\n",
            "377/377 [==============================] - 6s 16ms/step - loss: 0.8128 - accuracy: 0.7854\n",
            "Epoch 76/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.7519 - accuracy: 0.8015\n",
            "Epoch 77/500\n",
            "377/377 [==============================] - 5s 13ms/step - loss: 0.7242 - accuracy: 0.8089\n",
            "Epoch 78/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.7197 - accuracy: 0.8104\n",
            "Epoch 79/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.7160 - accuracy: 0.8139\n",
            "Epoch 80/500\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.7427 - accuracy: 0.8015\n",
            "Epoch 81/500\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.8603 - accuracy: 0.7726\n",
            "Epoch 82/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.9283 - accuracy: 0.7554\n",
            "Epoch 83/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.9511 - accuracy: 0.7476\n",
            "Epoch 84/500\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.9609 - accuracy: 0.7446\n",
            "Epoch 85/500\n",
            "377/377 [==============================] - 4s 11ms/step - loss: 0.9253 - accuracy: 0.7564\n",
            "Epoch 86/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.8662 - accuracy: 0.7702\n",
            "Epoch 87/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.8023 - accuracy: 0.7862\n",
            "Epoch 88/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.7615 - accuracy: 0.7971\n",
            "Epoch 89/500\n",
            "377/377 [==============================] - 4s 11ms/step - loss: 0.7308 - accuracy: 0.8072\n",
            "Epoch 90/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.7424 - accuracy: 0.8035\n",
            "Epoch 91/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.7508 - accuracy: 0.8015\n",
            "Epoch 92/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.7685 - accuracy: 0.7961\n",
            "Epoch 93/500\n",
            "377/377 [==============================] - 5s 13ms/step - loss: 0.7980 - accuracy: 0.7861\n",
            "Epoch 94/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.8820 - accuracy: 0.7655\n",
            "Epoch 95/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.9542 - accuracy: 0.7558\n",
            "Epoch 96/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.9351 - accuracy: 0.7577\n",
            "Epoch 97/500\n",
            "377/377 [==============================] - 5s 12ms/step - loss: 0.8860 - accuracy: 0.7648\n",
            "Epoch 98/500\n",
            "377/377 [==============================] - 4s 9ms/step - loss: 0.8130 - accuracy: 0.7843\n",
            "Epoch 99/500\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.7776 - accuracy: 0.7882\n",
            "Epoch 100/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.7444 - accuracy: 0.8023\n",
            "Epoch 101/500\n",
            "377/377 [==============================] - 5s 12ms/step - loss: 0.7462 - accuracy: 0.8005\n",
            "Epoch 102/500\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.7601 - accuracy: 0.7972\n",
            "Epoch 103/500\n",
            "377/377 [==============================] - 4s 9ms/step - loss: 0.7881 - accuracy: 0.7877\n",
            "Epoch 104/500\n",
            "377/377 [==============================] - 4s 9ms/step - loss: 0.8074 - accuracy: 0.7821\n",
            "Epoch 105/500\n",
            "377/377 [==============================] - 5s 12ms/step - loss: 0.8549 - accuracy: 0.7696\n",
            "Epoch 106/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.8759 - accuracy: 0.7655\n",
            "Epoch 107/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.8336 - accuracy: 0.7813\n",
            "Epoch 108/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.8358 - accuracy: 0.7759\n",
            "Epoch 109/500\n",
            "377/377 [==============================] - 5s 12ms/step - loss: 0.7942 - accuracy: 0.7880\n",
            "Epoch 110/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.8374 - accuracy: 0.7843\n",
            "Epoch 111/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.8282 - accuracy: 0.7817\n",
            "Epoch 112/500\n",
            "377/377 [==============================] - 4s 9ms/step - loss: 0.8146 - accuracy: 0.7809\n",
            "Epoch 113/500\n",
            "377/377 [==============================] - 5s 12ms/step - loss: 0.8167 - accuracy: 0.7830\n",
            "Epoch 114/500\n",
            "377/377 [==============================] - 5s 12ms/step - loss: 0.8184 - accuracy: 0.7804\n",
            "Epoch 115/500\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.8154 - accuracy: 0.7849\n",
            "Epoch 116/500\n",
            "377/377 [==============================] - 4s 11ms/step - loss: 0.8132 - accuracy: 0.7839\n",
            "Epoch 117/500\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.8335 - accuracy: 0.7765\n",
            "Epoch 118/500\n",
            "377/377 [==============================] - 4s 9ms/step - loss: 0.8516 - accuracy: 0.7750\n",
            "Epoch 119/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.8269 - accuracy: 0.7809\n",
            "Epoch 120/500\n",
            "377/377 [==============================] - 4s 12ms/step - loss: 0.7782 - accuracy: 0.7956\n",
            "Epoch 121/500\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.7683 - accuracy: 0.7987\n",
            "Epoch 122/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.7725 - accuracy: 0.7951\n",
            "Epoch 123/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.7393 - accuracy: 0.8009\n",
            "Epoch 124/500\n",
            "377/377 [==============================] - 4s 12ms/step - loss: 0.7693 - accuracy: 0.7937\n",
            "Epoch 125/500\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.8316 - accuracy: 0.7811\n",
            "Epoch 126/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.8696 - accuracy: 0.7731\n",
            "Epoch 127/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.8044 - accuracy: 0.7833\n",
            "Epoch 128/500\n",
            "377/377 [==============================] - 4s 12ms/step - loss: 0.7899 - accuracy: 0.7934\n",
            "Epoch 129/500\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.7770 - accuracy: 0.7946\n",
            "Epoch 130/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.7822 - accuracy: 0.7922\n",
            "Epoch 131/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.7703 - accuracy: 0.7928\n",
            "Epoch 132/500\n",
            "377/377 [==============================] - 4s 11ms/step - loss: 0.7282 - accuracy: 0.8048\n",
            "Epoch 133/500\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.7421 - accuracy: 0.7992\n",
            "Epoch 134/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.7547 - accuracy: 0.8009\n",
            "Epoch 135/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.8061 - accuracy: 0.7856\n",
            "Epoch 136/500\n",
            "377/377 [==============================] - 5s 12ms/step - loss: 0.8126 - accuracy: 0.7851\n",
            "Epoch 137/500\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.8057 - accuracy: 0.7847\n",
            "Epoch 138/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.7729 - accuracy: 0.7953\n",
            "Epoch 139/500\n",
            "377/377 [==============================] - 4s 9ms/step - loss: 0.7469 - accuracy: 0.8019\n",
            "Epoch 140/500\n",
            "377/377 [==============================] - 5s 12ms/step - loss: 0.7595 - accuracy: 0.7981\n",
            "Epoch 141/500\n",
            "377/377 [==============================] - 4s 9ms/step - loss: 0.7446 - accuracy: 0.8012\n",
            "Epoch 142/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.7433 - accuracy: 0.8009\n",
            "Epoch 143/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.7525 - accuracy: 0.7996\n",
            "Epoch 144/500\n",
            "377/377 [==============================] - 4s 12ms/step - loss: 0.7597 - accuracy: 0.7943\n",
            "Epoch 145/500\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.7645 - accuracy: 0.7994\n",
            "Epoch 146/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.7933 - accuracy: 0.7877\n",
            "Epoch 147/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.7817 - accuracy: 0.7897\n",
            "Epoch 148/500\n",
            "377/377 [==============================] - 4s 11ms/step - loss: 0.7814 - accuracy: 0.7959\n",
            "Epoch 149/500\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.7643 - accuracy: 0.7952\n",
            "Epoch 150/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.7626 - accuracy: 0.7962\n",
            "Epoch 151/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.7549 - accuracy: 0.7991\n",
            "Epoch 152/500\n",
            "377/377 [==============================] - 4s 11ms/step - loss: 0.7690 - accuracy: 0.7948\n",
            "Epoch 153/500\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.7335 - accuracy: 0.8021\n",
            "Epoch 154/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.7281 - accuracy: 0.8046\n",
            "Epoch 155/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.7142 - accuracy: 0.8089\n",
            "Epoch 156/500\n",
            "377/377 [==============================] - 4s 12ms/step - loss: 0.6998 - accuracy: 0.8128\n",
            "Epoch 157/500\n",
            "377/377 [==============================] - 4s 12ms/step - loss: 0.7010 - accuracy: 0.8104\n",
            "Epoch 158/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.7084 - accuracy: 0.8117\n",
            "Epoch 159/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.7306 - accuracy: 0.8065\n",
            "Epoch 160/500\n",
            "377/377 [==============================] - 4s 12ms/step - loss: 0.8051 - accuracy: 0.7853\n",
            "Epoch 161/500\n",
            "377/377 [==============================] - 4s 9ms/step - loss: 0.8583 - accuracy: 0.7744\n",
            "Epoch 162/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.8375 - accuracy: 0.7827\n",
            "Epoch 163/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.7974 - accuracy: 0.7885\n",
            "Epoch 164/500\n",
            "377/377 [==============================] - 4s 12ms/step - loss: 0.7592 - accuracy: 0.7961\n",
            "Epoch 165/500\n",
            "377/377 [==============================] - 4s 9ms/step - loss: 0.7563 - accuracy: 0.7978\n",
            "Epoch 166/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.7165 - accuracy: 0.8086\n",
            "Epoch 167/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.7043 - accuracy: 0.8140\n",
            "Epoch 168/500\n",
            "377/377 [==============================] - 4s 11ms/step - loss: 0.6851 - accuracy: 0.8162\n",
            "Epoch 169/500\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.6560 - accuracy: 0.8244\n",
            "Epoch 170/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.6630 - accuracy: 0.8242\n",
            "Epoch 171/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.6815 - accuracy: 0.8176\n",
            "Epoch 172/500\n",
            "377/377 [==============================] - 4s 11ms/step - loss: 0.7447 - accuracy: 0.8012\n",
            "Epoch 173/500\n",
            "377/377 [==============================] - 4s 9ms/step - loss: 0.7551 - accuracy: 0.7995\n",
            "Epoch 174/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.7794 - accuracy: 0.7908\n",
            "Epoch 175/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.7716 - accuracy: 0.7918\n",
            "Epoch 176/500\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.7856 - accuracy: 0.7948\n",
            "Epoch 177/500\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.7751 - accuracy: 0.7962\n",
            "Epoch 178/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.7422 - accuracy: 0.8020\n",
            "Epoch 179/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.7167 - accuracy: 0.8101\n",
            "Epoch 180/500\n",
            "377/377 [==============================] - 4s 11ms/step - loss: 0.7197 - accuracy: 0.8064\n",
            "Epoch 181/500\n",
            "377/377 [==============================] - 6s 15ms/step - loss: 0.7045 - accuracy: 0.8138\n",
            "Epoch 182/500\n",
            "377/377 [==============================] - 4s 9ms/step - loss: 0.6920 - accuracy: 0.8172\n",
            "Epoch 183/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.6858 - accuracy: 0.8171\n",
            "Epoch 184/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.6881 - accuracy: 0.8160\n",
            "Epoch 185/500\n",
            "377/377 [==============================] - 5s 12ms/step - loss: 0.7117 - accuracy: 0.8119\n",
            "Epoch 186/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.7317 - accuracy: 0.8029\n",
            "Epoch 187/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.7763 - accuracy: 0.7933\n",
            "Epoch 188/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.8598 - accuracy: 0.7774\n",
            "Epoch 189/500\n",
            "377/377 [==============================] - 4s 12ms/step - loss: 0.8511 - accuracy: 0.7789\n",
            "Epoch 190/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.7815 - accuracy: 0.7949\n",
            "Epoch 191/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.7406 - accuracy: 0.8050\n",
            "Epoch 192/500\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.7138 - accuracy: 0.8157\n",
            "Epoch 193/500\n",
            "377/377 [==============================] - 4s 11ms/step - loss: 0.6966 - accuracy: 0.8164\n",
            "Epoch 194/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.6899 - accuracy: 0.8168\n",
            "Epoch 195/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6822 - accuracy: 0.8202\n",
            "Epoch 196/500\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.7090 - accuracy: 0.8105\n",
            "Epoch 197/500\n",
            "377/377 [==============================] - 4s 11ms/step - loss: 0.7094 - accuracy: 0.8128\n",
            "Epoch 198/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.6935 - accuracy: 0.8136\n",
            "Epoch 199/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.7141 - accuracy: 0.8093\n",
            "Epoch 200/500\n",
            "377/377 [==============================] - 4s 11ms/step - loss: 0.7467 - accuracy: 0.8059\n",
            "Epoch 201/500\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.7420 - accuracy: 0.8055\n",
            "Epoch 202/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.7251 - accuracy: 0.8088\n",
            "Epoch 203/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.7538 - accuracy: 0.7995\n",
            "Epoch 204/500\n",
            "377/377 [==============================] - 4s 12ms/step - loss: 0.7568 - accuracy: 0.8020\n",
            "Epoch 205/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.7422 - accuracy: 0.8066\n",
            "Epoch 206/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.7148 - accuracy: 0.8110\n",
            "Epoch 207/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.7088 - accuracy: 0.8125\n",
            "Epoch 208/500\n",
            "377/377 [==============================] - 5s 12ms/step - loss: 0.8225 - accuracy: 0.7905\n",
            "Epoch 209/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.8014 - accuracy: 0.7899\n",
            "Epoch 210/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.7600 - accuracy: 0.7998\n",
            "Epoch 211/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.7155 - accuracy: 0.8115\n",
            "Epoch 212/500\n",
            "377/377 [==============================] - 4s 11ms/step - loss: 0.7677 - accuracy: 0.8095\n",
            "Epoch 213/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.7577 - accuracy: 0.8065\n",
            "Epoch 214/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.7263 - accuracy: 0.8104\n",
            "Epoch 215/500\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.7240 - accuracy: 0.8125\n",
            "Epoch 216/500\n",
            "377/377 [==============================] - 4s 11ms/step - loss: 0.7071 - accuracy: 0.8130\n",
            "Epoch 217/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6975 - accuracy: 0.8190\n",
            "Epoch 218/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6795 - accuracy: 0.8207\n",
            "Epoch 219/500\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.6789 - accuracy: 0.8183\n",
            "Epoch 220/500\n",
            "377/377 [==============================] - 4s 11ms/step - loss: 0.6970 - accuracy: 0.8169\n",
            "Epoch 221/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.7337 - accuracy: 0.8084\n",
            "Epoch 222/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.7975 - accuracy: 0.7991\n",
            "Epoch 223/500\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.7946 - accuracy: 0.7974\n",
            "Epoch 224/500\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.9757 - accuracy: 0.7744\n",
            "Epoch 225/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.8846 - accuracy: 0.7755\n",
            "Epoch 226/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.7779 - accuracy: 0.7975\n",
            "Epoch 227/500\n",
            "377/377 [==============================] - 4s 11ms/step - loss: 0.7602 - accuracy: 0.8047\n",
            "Epoch 228/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.7239 - accuracy: 0.8128\n",
            "Epoch 229/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.7061 - accuracy: 0.8154\n",
            "Epoch 230/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.6805 - accuracy: 0.8248\n",
            "Epoch 231/500\n",
            "377/377 [==============================] - 5s 12ms/step - loss: 0.6896 - accuracy: 0.8234\n",
            "Epoch 232/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.6969 - accuracy: 0.8197\n",
            "Epoch 233/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.7684 - accuracy: 0.8073\n",
            "Epoch 234/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.7697 - accuracy: 0.8008\n",
            "Epoch 235/500\n",
            "377/377 [==============================] - 4s 11ms/step - loss: 0.9213 - accuracy: 0.7805\n",
            "Epoch 236/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.8949 - accuracy: 0.7755\n",
            "Epoch 237/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.8207 - accuracy: 0.7927\n",
            "Epoch 238/500\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.7622 - accuracy: 0.8000\n",
            "Epoch 239/500\n",
            "377/377 [==============================] - 4s 11ms/step - loss: 0.7433 - accuracy: 0.8070\n",
            "Epoch 240/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.7872 - accuracy: 0.8028\n",
            "Epoch 241/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.7602 - accuracy: 0.8036\n",
            "Epoch 242/500\n",
            "377/377 [==============================] - 4s 11ms/step - loss: 0.7399 - accuracy: 0.8067\n",
            "Epoch 243/500\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.7409 - accuracy: 0.8084\n",
            "Epoch 244/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.7301 - accuracy: 0.8099\n",
            "Epoch 245/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.7177 - accuracy: 0.8141\n",
            "Epoch 246/500\n",
            "377/377 [==============================] - 4s 11ms/step - loss: 0.6994 - accuracy: 0.8186\n",
            "Epoch 247/500\n",
            "377/377 [==============================] - 4s 9ms/step - loss: 0.6974 - accuracy: 0.8219\n",
            "Epoch 248/500\n",
            "377/377 [==============================] - 4s 12ms/step - loss: 0.6905 - accuracy: 0.8188\n",
            "Epoch 249/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.6877 - accuracy: 0.8205\n",
            "Epoch 250/500\n",
            "377/377 [==============================] - 4s 11ms/step - loss: 0.6979 - accuracy: 0.8177\n",
            "Epoch 251/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.6933 - accuracy: 0.8198\n",
            "Epoch 252/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.7462 - accuracy: 0.8107\n",
            "Epoch 253/500\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.8346 - accuracy: 0.7866\n",
            "Epoch 254/500\n",
            "377/377 [==============================] - 4s 11ms/step - loss: 0.8981 - accuracy: 0.7807\n",
            "Epoch 255/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 1.0933 - accuracy: 0.7473\n",
            "Epoch 256/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.8987 - accuracy: 0.7768\n",
            "Epoch 257/500\n",
            "377/377 [==============================] - 4s 11ms/step - loss: 0.8048 - accuracy: 0.7940\n",
            "Epoch 258/500\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.7677 - accuracy: 0.8045\n",
            "Epoch 259/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.7229 - accuracy: 0.8151\n",
            "Epoch 260/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.6993 - accuracy: 0.8223\n",
            "Epoch 261/500\n",
            "377/377 [==============================] - 5s 12ms/step - loss: 0.6999 - accuracy: 0.8229\n",
            "Epoch 262/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.6979 - accuracy: 0.8225\n",
            "Epoch 263/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.7148 - accuracy: 0.8164\n",
            "Epoch 264/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.7678 - accuracy: 0.8076\n",
            "Epoch 265/500\n",
            "377/377 [==============================] - 5s 12ms/step - loss: 0.8831 - accuracy: 0.7839\n",
            "Epoch 266/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.8553 - accuracy: 0.7858\n",
            "Epoch 267/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.8391 - accuracy: 0.7902\n",
            "Epoch 268/500\n",
            "377/377 [==============================] - 4s 11ms/step - loss: 0.8147 - accuracy: 0.7932\n",
            "Epoch 269/500\n",
            "377/377 [==============================] - 4s 11ms/step - loss: 0.8105 - accuracy: 0.7931\n",
            "Epoch 270/500\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.8278 - accuracy: 0.7940\n",
            "Epoch 271/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.8072 - accuracy: 0.8002\n",
            "Epoch 272/500\n",
            "377/377 [==============================] - 4s 12ms/step - loss: 0.7902 - accuracy: 0.8022\n",
            "Epoch 273/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.7433 - accuracy: 0.8100\n",
            "Epoch 274/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.7545 - accuracy: 0.8140\n",
            "Epoch 275/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.7721 - accuracy: 0.8102\n",
            "Epoch 276/500\n",
            "377/377 [==============================] - 5s 12ms/step - loss: 0.7247 - accuracy: 0.8155\n",
            "Epoch 277/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.7078 - accuracy: 0.8192\n",
            "Epoch 278/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.7154 - accuracy: 0.8202\n",
            "Epoch 279/500\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.7417 - accuracy: 0.8126\n",
            "Epoch 280/500\n",
            "377/377 [==============================] - 4s 11ms/step - loss: 0.7771 - accuracy: 0.8024\n",
            "Epoch 281/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.7796 - accuracy: 0.8034\n",
            "Epoch 282/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.7784 - accuracy: 0.8015\n",
            "Epoch 283/500\n",
            "377/377 [==============================] - 4s 11ms/step - loss: 0.7671 - accuracy: 0.8040\n",
            "Epoch 284/500\n",
            "377/377 [==============================] - 4s 11ms/step - loss: 0.7405 - accuracy: 0.8123\n",
            "Epoch 285/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.7442 - accuracy: 0.8111\n",
            "Epoch 286/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.7622 - accuracy: 0.8040\n",
            "Epoch 287/500\n",
            "377/377 [==============================] - 5s 12ms/step - loss: 0.7667 - accuracy: 0.8040\n",
            "Epoch 288/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.7375 - accuracy: 0.8066\n",
            "Epoch 289/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.7484 - accuracy: 0.8086\n",
            "Epoch 290/500\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.7400 - accuracy: 0.8122\n",
            "Epoch 291/500\n",
            "377/377 [==============================] - 4s 12ms/step - loss: 0.7369 - accuracy: 0.8084\n",
            "Epoch 292/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.7217 - accuracy: 0.8153\n",
            "Epoch 293/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.7203 - accuracy: 0.8162\n",
            "Epoch 294/500\n",
            "377/377 [==============================] - 4s 11ms/step - loss: 0.7022 - accuracy: 0.8172\n",
            "Epoch 295/500\n",
            "377/377 [==============================] - 4s 11ms/step - loss: 0.7292 - accuracy: 0.8181\n",
            "Epoch 296/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.7248 - accuracy: 0.8209\n",
            "Epoch 297/500\n",
            " 89/377 [======>.......................] - ETA: 2s - loss: 0.5881 - accuracy: 0.8420"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-69-bd21445c7b92>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1805\u001b[0m                         ):\n\u001b[1;32m   1806\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1807\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1808\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    869\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.xlabel('epoche')\n",
        "plt.ylabel('accuracy')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Fy9veTwYidL2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "fmCn6Ze6ZBeZ",
        "inA2gZ6XZakj",
        "WUzBwOLD-hV6"
      ],
      "mount_file_id": "1ZJkQ0q6Hy6RGb40zcerQZeCluW3F_Qm8",
      "authorship_tag": "ABX9TyPfJj9S14w9/x3oEyAYboLI",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}